{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', 'there', 'good', 'man!'],\n",
       " ['It', 'is', 'quite', 'windy', 'in', 'London'],\n",
       " ['How', 'is', 'the', 'weather', 'today?']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "corpus = [\n",
    "    \"Hello there good man!\",\n",
    "    \"It is quite windy in London\",\n",
    "    \"How is the weather today?\"\n",
    "]\n",
    "\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['windy', 'London']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"windy London\"\n",
    "tokenized_query = query.split(\" \")\n",
    "tokenized_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.93729472, 0.        ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_scores = bm25.get_scores(tokenized_query)\n",
    "doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It is quite windy in London',\n",
       " 'How is the weather today?',\n",
       " 'Hello there good man!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.get_top_n(tokenized_query, corpus, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from unidecode import unidecode\n",
    "\n",
    "ENTITIES = [\"@sys-number\", \"@Datas\"]\n",
    "stopwordsNLTK = nltk.corpus.stopwords.words('portuguese')\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "def analyzer(text):\n",
    "    text = unidecode(text) # remove accent\n",
    "    for punctuation in [\".\", \"!\", \"?\", \",\", \";\"]:\n",
    "        text = text.replace(punctuation, \"\")\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    without_stopwords = [word for word in text.split() if word not in stopwordsNLTK]\n",
    "    return without_stopwords\n",
    "\n",
    "def apply_stemmer(tokens):\n",
    "    stemmed_words = []\n",
    "    for token in tokens:\n",
    "        if token not in ENTITIES:\n",
    "            stemmed_words.append(stemmer.stem(token.lower()))\n",
    "        else:\n",
    "            stemmed_words.append(token)\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Consultar_ContasPagas = [\"A minha conta de @Datas está disponível para consulta?\",\n",
    "                        \"Como consultar todas as minhas contas pagas?\",\n",
    "                        \"Como consulto minha conta do dia @Datas?\",\n",
    "                        \"Como faço para consultar minhas contas antigas?\",\n",
    "                        \"como vejo os boletos que paguei?\",\n",
    "                        \"consigo ver meus pagamentos?\",\n",
    "                        \"Dá pra ver todas as contas que paguei?\",\n",
    "                        \"extrato de pagamento de conta\",\n",
    "                        \"Gostaria de consultar minhas faturas pagas\"]\n",
    "\n",
    "Emitir_SegundaViaConta = [\"ACABEI DE DANIFICAR A CONTA NAO CONSIGO 2 VIA PELO SITE\",\n",
    "                         \"Como consigo pagar minha conta atrasada?\",\n",
    "                         \"como eu emito um novo boleto?\",\n",
    "                         \"como faço para gerar 2ª via de conta?\",\n",
    "                         \"Como tirar uma nova conta deste mês?\",\n",
    "                         \"poderia me ajudar a pagar uma conta atrasada?\",\n",
    "                         \"preciso imprimir segunda via de boleto\",\n",
    "                         \"quero pagar meu boleto atrasado\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A minha conta de @Datas esta disponivel para consulta Como consultar todas as minhas contas pagas Como consulto minha conta do dia @Datas Como faco para consultar minhas contas antigas como vejo os boletos que paguei consigo ver meus pagamentos Da pra ver todas as contas que paguei extrato de pagamento de conta Gostaria de consultar minhas faturas pagas'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Consultar_ContasPagas_text = analyzer(\" \".join(Consultar_ContasPagas))\n",
    "Consultar_ContasPagas_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACABEI DE DANIFICAR A CONTA NAO CONSIGO 2 VIA PELO SITE Como consigo pagar minha conta atrasada como eu emito um novo boleto como faco para gerar 2a via de conta Como tirar uma nova conta deste mes poderia me ajudar a pagar uma conta atrasada preciso imprimir segunda via de boleto quero pagar meu boleto atrasado'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Emitir_SegundaViaConta_text = analyzer(\" \".join(Emitir_SegundaViaConta))\n",
    "Emitir_SegundaViaConta_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'cont',\n",
       " '@Datas',\n",
       " 'disponi',\n",
       " 'consult',\n",
       " 'com',\n",
       " 'consult',\n",
       " 'tod',\n",
       " 'cont',\n",
       " 'pag',\n",
       " 'com',\n",
       " 'consult',\n",
       " 'cont',\n",
       " 'dia',\n",
       " '@Datas',\n",
       " 'com',\n",
       " 'fac',\n",
       " 'consult',\n",
       " 'cont',\n",
       " 'antig',\n",
       " 'vej',\n",
       " 'bolet',\n",
       " 'pag',\n",
       " 'consig',\n",
       " 'ver',\n",
       " 'pag',\n",
       " 'da',\n",
       " 'pra',\n",
       " 'ver',\n",
       " 'tod',\n",
       " 'cont',\n",
       " 'pag',\n",
       " 'extrat',\n",
       " 'pag',\n",
       " 'cont',\n",
       " 'gost',\n",
       " 'consult',\n",
       " 'fatur',\n",
       " 'pag']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens1 = remove_stopwords(Consultar_ContasPagas_text)\n",
    "reduced_words = apply_stemmer(tokens1)\n",
    "reduced_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'cont',\n",
       " '@Datas',\n",
       " 'disponi',\n",
       " 'consult',\n",
       " 'com',\n",
       " 'consult',\n",
       " 'tod',\n",
       " 'cont',\n",
       " 'pag',\n",
       " 'com',\n",
       " 'consult',\n",
       " 'cont',\n",
       " 'dia',\n",
       " '@Datas',\n",
       " 'com',\n",
       " 'fac',\n",
       " 'consult',\n",
       " 'cont',\n",
       " 'antig',\n",
       " 'vej',\n",
       " 'bolet',\n",
       " 'pag',\n",
       " 'consig',\n",
       " 'ver',\n",
       " 'pag',\n",
       " 'da',\n",
       " 'pra',\n",
       " 'ver',\n",
       " 'tod',\n",
       " 'cont',\n",
       " 'pag',\n",
       " 'extrat',\n",
       " 'pag',\n",
       " 'cont',\n",
       " 'gost',\n",
       " 'consult',\n",
       " 'fatur',\n",
       " 'pag']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2 = remove_stopwords(Consultar_ContasPagas_text)\n",
    "reduced_words2 = apply_stemmer(tokens2)\n",
    "reduced_words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\" \".join(reduced_words), \" \".join(reduced_words2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0877058 , 0.0877058 , 0.26311741, 0.0877058 , 0.43852901,\n",
       "        0.52623481, 0.0877058 , 0.1754116 , 0.0877058 , 0.0877058 ,\n",
       "        0.0877058 , 0.0877058 , 0.0877058 , 0.0877058 , 0.52623481,\n",
       "        0.0877058 , 0.1754116 , 0.0877058 , 0.1754116 ],\n",
       "       [0.0877058 , 0.0877058 , 0.26311741, 0.0877058 , 0.43852901,\n",
       "        0.52623481, 0.0877058 , 0.1754116 , 0.0877058 , 0.0877058 ,\n",
       "        0.0877058 , 0.0877058 , 0.0877058 , 0.0877058 , 0.52623481,\n",
       "        0.0877058 , 0.1754116 , 0.0877058 , 0.1754116 ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td_idf_docs = vectorizer.fit_transform(corpus).toarray()\n",
    "td_idf_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if index exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elastic_db.elasticsearch import elastic_conection\n",
    "\n",
    "es = elastic_conection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.exists(index=\"nlp_model-dc1e7b3d-9137-4a20-a99c-d0d2029ef170-01012021122212\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'took': 0,\n",
       " 'timed_out': False,\n",
       " 'total': 0,\n",
       " 'deleted': 0,\n",
       " 'batches': 0,\n",
       " 'version_conflicts': 0,\n",
       " 'noops': 0,\n",
       " 'retries': {'bulk': 0, 'search': 0},\n",
       " 'throttled_millis': 0,\n",
       " 'requests_per_second': -1.0,\n",
       " 'throttled_until_millis': 0,\n",
       " 'failures': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.delete_by_query(index=\"nlp_model-dc1e7b3d-9137-4a20-a99c-d0d2029ef170\", body={\n",
    "                                                          \"query\": {\n",
    "                                                            \"term\": {\n",
    "                                                              \"intent\": \"Abono\"\n",
    "                                                            }\n",
    "                                                          }\n",
    "                                                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "NotFoundError(404, '{\"_index\":\"nlp_model-dc1e7b3d-9137-4a20-a99c-d0d2029ef170-01012021133423\",\"_type\":\"_doc\",\"_id\":\"GgHavnYB2VES4V58Lm8\",\"found\":false}')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1d824a103763>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m es.get(index=\"nlp_model-dc1e7b3d-9137-4a20-a99c-d0d2029ef170-01012021133423\",\n\u001b[0;32m----> 2\u001b[0;31m       id=\"GgHavnYB2VES4V58Lm8\")\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/elasticsearch/client/utils.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/elasticsearch/client/__init__.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, index, id, doc_type, params)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty value passed for a required argument.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         return self.transport.perform_request(\n\u001b[0;32m--> 507\u001b[0;31m             \u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_make_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m         )\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/elasticsearch/transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                     \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m                 )\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/elasticsearch/connection/http_urllib3.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             )\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         self.log_request_success(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/elasticsearch/connection/base.py\u001b[0m in \u001b[0;36m_raise_error\u001b[0;34m(self, status_code, raw_data)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         raise HTTP_EXCEPTIONS.get(status_code, TransportError)(\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         )\n",
      "\u001b[0;31mNotFoundError\u001b[0m: NotFoundError(404, '{\"_index\":\"nlp_model-dc1e7b3d-9137-4a20-a99c-d0d2029ef170-01012021133423\",\"_type\":\"_doc\",\"_id\":\"GgHavnYB2VES4V58Lm8\",\"found\":false}')"
     ]
    }
   ],
   "source": [
    "es.get(index=\"nlp_model-dc1e7b3d-9137-4a20-a99c-d0d2029ef170-01012021133423\",\n",
    "      id=\"GgHavnYB2VES4V58Lm8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " es.indices.delete(index=\"nlp_model-dc1e7b3d-9137-4a20-a99c-d0d2029ef170-01012021122212\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Elasticsearch\n",
    "\n",
    "Read [Classificação de texto facilitada com o Elasticsearch](https://www.elastic.co/pt/blog/text-classification-made-easy-with-elasticsearch)\n",
    "\n",
    "\n",
    "mapping:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"index\" : {\n",
    "      \"number_of_shards\" : 1,\n",
    "      \"number_of_replicas\" : 0\n",
    "    },\n",
    "    \"analysis\": {\n",
    "      \"filter\": {\n",
    "        \"portuguese_stop\":{\n",
    "               \"type\": \"stop\",\n",
    "               \"stopwords\": \"_portuguese_\"\n",
    "            },\n",
    "        \"my_stemmer\": {\n",
    "          \"type\": \"stemmer\",\n",
    "          \"language\": \"portuguese_rslp\"\n",
    "        }\n",
    "      },\n",
    "    \"analyzer\": {\n",
    "        \"sys-date\": {\n",
    "          \"type\": \"pattern\",\n",
    "          \"pattern\":   [\"[0-9]{2}[/-][0-9]{2}[/-][0-9]{2,4}\",  \"((\\d{1,2}|\\dº|primeiro)( de )?(janeiro|fevereiro|março|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro))( de \\d{2,4})?\"], \n",
    "          \"lowercase\": true\n",
    "        }\n",
    "        \"examples_intent_analyzer\":{ \n",
    "               \"type\":\"custom\",\n",
    "               \"tokenizer\":\"standard\",\n",
    "               \"char_filter\":  [ \"html_strip\" ],\n",
    "               \"filter\":[\n",
    "                  \"lowercase\",\n",
    "                  \"asciifolding\",\n",
    "                  \"portuguese_stop\"\n",
    "               ]\n",
    "            }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": { \n",
    "    \"properties\":{\n",
    "       \"examples\":{\n",
    "          \"type\":\"text\",\n",
    "          \"analyzer\":\"examples_intent_analyzer\",\n",
    "          \"term_vector\": \"yes\"\n",
    "       },\n",
    "       \"intent\":{\"type\": \"keyword\"}\n",
    "    }\n",
    "  }\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "  \"settings\": {\n",
    "    \"index\" : {\n",
    "      \"number_of_shards\" : 1,\n",
    "      \"number_of_replicas\" : 0\n",
    "    },\n",
    "    \"analysis\": {\n",
    "      \"filter\": {\n",
    "        \"sys-date\" : {\n",
    "               \"type\" : \"pattern_replace\",\n",
    "               \"lowercase\": True,\n",
    "               \"pattern\" : \"((\\d{1,2}|\\dº|primeiro)( de )?(janeiro|fevereiro|março|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro))( de \\d{2,4})?\",\n",
    "               \"replacement\": \"date\"\n",
    "        },\n",
    "        \"portuguese_stop\":{\n",
    "               \"type\": \"stop\",\n",
    "               \"stopwords\": \"_portuguese_\"\n",
    "            },\n",
    "        \"my_stemmer\": {\n",
    "          \"type\": \"stemmer\",\n",
    "          \"language\": \"portuguese_rslp\"\n",
    "        }\n",
    "      },\n",
    "    \"analyzer\": {\n",
    "        \"examples_intent_analyzer\":{ \n",
    "               \"type\":\"custom\",\n",
    "               \"tokenizer\":\"standard\",\n",
    "               \"char_filter\":  [ \"html_strip\" ],\n",
    "               \"filter\":[\n",
    "                  \"lowercase\",\n",
    "                  \"sys-date\",\n",
    "                  \"asciifolding\",\n",
    "                  \"portuguese_stop\",\n",
    "                  \"my_stemmer\"\n",
    "               ]\n",
    "            }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": { \n",
    "    \"properties\":{\n",
    "       \"examples\":{\n",
    "          \"type\":\"text\",\n",
    "          \"analyzer\":\"standard\"\n",
    "       },\n",
    "       \"intent\":{\"type\": \"keyword\"}\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elastic_db.elasticsearch import elastic_conection, NLPmodelIndex\n",
    "\n",
    "es = elastic_conection()\n",
    "index = NLPmodelIndex(es, workspace_id=\"dc1e7b3d-9137-4a20-a99c-d0d2029ef170\")\n",
    "\n",
    "# es = elastic_conection()\n",
    "# index_name = 'yara-bot-yarin-dc1e7b3d-9137-4a20-a99c-d0d2029ef170'\n",
    "# try:\n",
    "#     es.indices.create(index=index_name,  body=mapping)\n",
    "# except:\n",
    "#     es.indices.delete(index=index_name)\n",
    "#     es.indices.create(index=index_name,  body=mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.index_exist(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nlp_model-dc1e7b3d-9137-4a20-a99c-d0d2029ef170-25122020150019'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.index_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nlp_model-dc1e7b3d-9137-4a20-a99c-d0d2029ef170'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.index_alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'language': 'pt-br', 'model_kind': 'BM25', 'BM25': {'b': 0.75, 'k1': 1.2}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.model_recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'shards_acknowledged': True,\n",
       " 'index': 'nlp_model-dc1e7b3d-9137-4a20-a99c-d0d2029ef170-25122020150019'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.create_index(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.create_index(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " {'_index': 'nlp_model-dc1e7b3d-9137-4a20-a99c-d0d2029ef170-25122020150019',\n",
       "  '_type': '_doc',\n",
       "  '_id': 'dc1e7b3d-9137-4a20-a99c-d0d2029ef170',\n",
       "  '_version': 1,\n",
       "  'result': 'created',\n",
       "  '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n",
       "  '_seq_no': 0,\n",
       "  '_primary_term': 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.create_recipe(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'Recipe already exist!')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.create_recipe(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': 'Vizinho_Invadindo_Terreno',\n",
       " 'examples': [{'text': 'A unidade está sendo invadida. Quem pode ajudar?'},\n",
       "  {'text': 'Estamos com problema com vizinhos da unidade. Com quem posso falar?'},\n",
       "  {'text': 'Nosso terreno está sendo invadido pelo vizinho da unidade. O que fazer?'},\n",
       "  {'text': 'O vizinho da unidade está invadindo uma parte do imóvel da Yara. O que fazer?'}],\n",
       " 'description': ''}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file = open('/home/cloves/Downloads/skill-YARA_PRD.json', \"r\") \n",
    "workspace = json.load(file)\n",
    "workspace[\"intents\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': 'Vizinho_Invadindo_Terreno',\n",
       " 'examples': [{'text': 'A unidade está sendo invadida. Quem pode ajudar?'},\n",
       "  {'text': 'Estamos com problema com vizinhos da unidade. Com quem posso falar?'},\n",
       "  {'text': 'Nosso terreno está sendo invadido pelo vizinho da unidade. O que fazer?'},\n",
       "  {'text': 'O vizinho da unidade está invadindo uma parte do imóvel da Yara. O que fazer?'}],\n",
       " 'description': ''}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = '\\\n",
    "  {\"intent\": \"Vizinho_Invadindo_Terreno\",\\\n",
    "   \"examples\": [{\"text\": \"A unidade está sendo invadida. Quem pode ajudar?\"},\\\n",
    "                 {\"text\": \"Estamos com problema com vizinhos da unidade. Com quem posso falar?\"},\\\n",
    "                 {\"text\": \"Nosso terreno está sendo invadido pelo vizinho da unidade. O que fazer?\"},\\\n",
    "                 {\"text\": \"O vizinho da unidade está invadindo uma parte do imóvel da Yara. O que fazer?\"}\\\n",
    "    ],\\\n",
    "  \"description\": \"\"}'\n",
    "    \n",
    "json.loads(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in workspace[\"intents\"]:\n",
    "    index.add_intent(es, intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Adiantamento_e_QuandoReceber_13_Salario', 10.771127)\n",
      "('Data_Envio_Vale_RefeicaoAlimentacao', 6.4685035)\n",
      "('Data_Pagamento_Vale_RefeicaoAlimentação', 6.335941)\n",
      "('Contrato_Devolvido_Juridico', 6.32493)\n",
      "('Data_Pagamento_Salario', 6.283629)\n",
      "('Hora_Extra', 5.9060307)\n",
      "('Bonus_Mobilidade', 4.600733)\n",
      "('Duvidas_Funcoes_Cartao', 4.5996284)\n",
      "('Pagamento_Licenca_Maternidade', 4.4481616)\n",
      "('Custo_Movimentação_Estado', 4.3218856)\n"
     ]
    }
   ],
   "source": [
    "user_input = \"quando o 13º será pago?\"\n",
    "query = {\n",
    "    \"query\":{\n",
    "       \"more_like_this\":{\n",
    "          \"fields\":[\"examples.text\"],\n",
    "          \"like\":user_input,\n",
    "          \"min_term_freq\":1,\n",
    "          \"max_query_terms\":20\n",
    "       }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = es.search(index=index.index_name, body = query)\n",
    "for r in response[\"hits\"][\"hits\"]:\n",
    "    print((r[\"_source\"][\"intent\"], r[\"_score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isso aqui é um float 10.100000\n"
     ]
    }
   ],
   "source": [
    "print(\"Isso aqui é um float %f\"%10.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isso aqui é um float 10.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Isso aqui é um float {numero}\".format(numero=\"10.1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isso é um numero  10\n"
     ]
    }
   ],
   "source": [
    "print(\"isso é um numero \", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the text data with elastic\n",
    "\n",
    "\n",
    "Read [How scoring works in Elasticsearch](https://www.compose.com/articles/how-scoring-works-in-elasticsearch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "GET /_analyze\n",
    "{\n",
    "  \"char_filter\" : [\"html_strip\"],\n",
    "  \"tokenizer\" : \"whitespace\",\n",
    "  \"filter\" : [\"lowercase\", \"asciifolding\", {\"type\": \"stemmer\", \"language\": \"portuguese_rslp\"},{\"type\": \"stop\", \"stopwords\": [\"que\", \"com\", \",\", \".\", \"?\"]}],\n",
    "  \"text\" : \"Olá tudo bem? <h1>Eu vou bem</h2>, que bom, estava com saudades\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elastic_db.elasticsearch import elastic_conection\n",
    "\n",
    "es = elastic_conection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [{'token': 'rapozas',\n",
       "   'start_offset': 0,\n",
       "   'end_offset': 7,\n",
       "   'type': 'word',\n",
       "   'position': 0},\n",
       "  {'token': 'são',\n",
       "   'start_offset': 8,\n",
       "   'end_offset': 11,\n",
       "   'type': 'word',\n",
       "   'position': 1},\n",
       "  {'token': 'watchcachorros',\n",
       "   'start_offset': 12,\n",
       "   'end_offset': 21,\n",
       "   'type': 'word',\n",
       "   'position': 2}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from unidecode import unidecode\n",
    "\n",
    "# https://www.freeformatter.com/java-regex-tester.html#ad-output\n",
    "\n",
    "stopwordsNLTK = list(nltk.corpus.stopwords.words('portuguese'))\n",
    "\n",
    "input_text = \"<h1>olá tudo bem?</h1> 1º de janeiro, 25 de fevereiro, 15 de dezembro, primeiro de abril de 2020, 2 de junho de 2018\"\n",
    "input_text2 = \"olá bom dia 20/10/2020, 30-01-20\"\n",
    "\n",
    "query1 = {\n",
    "  \"char_filter\" : [\"html_strip\"],\n",
    "  \"tokenizer\" : \"whitespace\",\n",
    "  \"filter\" : [\n",
    "              \"lowercase\",\n",
    "              {\n",
    "               \"type\" : \"pattern_replace\",\n",
    "               \"pattern\" : \"[0-9]{2}[/-][0-9]{2}[/-][0-9]{2,4}\",\n",
    "               \"replacement\": \"date\"\n",
    "              },\n",
    "              \"asciifolding\",\n",
    "              #{\"type\": \"stemmer\", \"language\": \"portuguese_rslp\"},\n",
    "              {\"type\": \"stop\", \"stopwords\": stopwordsNLTK}],\n",
    "  \"text\" : input_text2\n",
    "}\n",
    "\n",
    "query2 = {\n",
    "  \"char_filter\" : [\"html_strip\"],\n",
    "  \"tokenizer\" : \"whitespace\",\n",
    "  \"filter\" : [\n",
    "              \"lowercase\",\n",
    "              {\n",
    "               \"type\" : \"pattern_replace\",\n",
    "               \"pattern\" : \"(([0-9]{1,2}|[0-9]º|primeiro)( de )?(janeiro\\b|fevereiro\\b|março\\b|abril\\b|maio\\b|junho\\b|julho\\b|agosto\\b|setembro\\b|outubro\\b|novembro\\b|dezembro\\b))( de [0-9]{2,4})?\",\n",
    "               \"replacement\": \"date\"\n",
    "              },\n",
    "              \"asciifolding\",\n",
    "              {\"type\": \"stop\", \"stopwords\": \"_portuguese_\"}],\n",
    "  \"text\" : input_text\n",
    "}\n",
    "\n",
    "\n",
    "query3 = {\n",
    "  \"tokenizer\": \"whitespace\",\n",
    "  \"filter\": [\n",
    "    {\n",
    "      \"type\": \"pattern_replace\",\n",
    "      \"pattern\": \"(cachorro)\",\n",
    "      \"replacement\": \"watch$1\"\n",
    "    }\n",
    "  ],\n",
    "  \"text\": \"rapozas são cachorros\"\n",
    "}\n",
    "index_name=\"nlp_model-dc1e7b3d-9137-4a20-a99c-d0d2029ef170\"\n",
    "result = es.indices.analyze(index=index_name, body=query3)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [{'token': 'como',\n",
       "   'start_offset': 0,\n",
       "   'end_offset': 4,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 0},\n",
       "  {'token': 'faço',\n",
       "   'start_offset': 5,\n",
       "   'end_offset': 9,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 1},\n",
       "  {'token': 'para',\n",
       "   'start_offset': 10,\n",
       "   'end_offset': 14,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 2},\n",
       "  {'token': 'ver',\n",
       "   'start_offset': 15,\n",
       "   'end_offset': 18,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 3},\n",
       "  {'token': 'os',\n",
       "   'start_offset': 19,\n",
       "   'end_offset': 21,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 4},\n",
       "  {'token': 'erros',\n",
       "   'start_offset': 22,\n",
       "   'end_offset': 27,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 5},\n",
       "  {'token': 'do',\n",
       "   'start_offset': 28,\n",
       "   'end_offset': 30,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 6},\n",
       "  {'token': 'meu',\n",
       "   'start_offset': 31,\n",
       "   'end_offset': 34,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 7},\n",
       "  {'token': 'ponto',\n",
       "   'start_offset': 35,\n",
       "   'end_offset': 40,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 8},\n",
       "  {'token': 'e',\n",
       "   'start_offset': 41,\n",
       "   'end_offset': 42,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 9},\n",
       "  {'token': 'tomar',\n",
       "   'start_offset': 43,\n",
       "   'end_offset': 48,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 10},\n",
       "  {'token': 'café',\n",
       "   'start_offset': 49,\n",
       "   'end_offset': 53,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 11}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = {\n",
    " \"analyzer\": \"standard\",\n",
    " \"text\": \"Como faço para ver os erros do meu ponto e tomar café?\"\n",
    "}\n",
    "result = es.indices.analyze(index=index_name, body=query)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Resolution\n",
    "\n",
    "* [Real time entity resolution with elasticsearch](https://www2.slideshare.net/o19s/real-time-entity-resolution-with-elasticsearch-haystack-2018?from_action=save)\n",
    "* [Named Entity Annotations in Elasticsearch](https://saskia-vola.com/named-entity-annotations-in-elasticsearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API\n",
    "\n",
    "[REST API Testing Strategy: What Exactly Should You Test?](https://www.sisense.com/blog/rest-api-testing-strategy-what-exactly-should-you-test/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(key_path, doc):\n",
    "    keys = key_path.split('.')\n",
    "    rv = doc\n",
    "    try:\n",
    "        for key in keys:\n",
    "            rv = rv[key]\n",
    "        return {keys[-1]: rv}\n",
    "    except KeyError:\n",
    "        return \"Field %s do not exist!\"%keys[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = {\n",
    "    \"customer_id\": \"yara\",\n",
    "    \"language\": \"pt-br\",\n",
    "    \"recipe\": {\n",
    "        \"BM25\": {\n",
    "            \"b\": 0.75,\n",
    "            \"k1\": 1.2\n",
    "        },\n",
    "        \"model_kind\": \"BM25\"\n",
    "    },\n",
    "    \"workspace_id\": \"dc1e7b3d-9137-4a20-a99c-d0d2029ef170\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k1': 1.2}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find(\"recipe.BM25.k1\", doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def convert_value_to_reight_type(value):\n",
    "    try:\n",
    "        value = json.loads(value)\n",
    "        return value\n",
    "    except:\n",
    "        try:\n",
    "            value = int(value)\n",
    "            return value\n",
    "        except:\n",
    "            try:\n",
    "                value = float(value)\n",
    "                return value\n",
    "            except:\n",
    "                if value.lower() == \"true\":\n",
    "                    return bool(value)\n",
    "                else:\n",
    "                    return value\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nome': 'cloves'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_value('{\"nome\": \"cloves\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(\"cloves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update(doc, \"recipe.BM25.k1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from elastic_db.elasticsearch import elastic_conection, NLPmodelIndex\n",
    "\n",
    "es = elastic_conection()\n",
    "index = NLPmodelIndex(es=es, workspace_id=\"dc1e7b3d-9137-4a20-a99c-d0d2029ef170\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ContactInfo:email': ['\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,}\\\\b',\n",
       "  \"^[a-zA-Z0-9.!#$%&'*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\\\\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*$\"],\n",
       " 'ContactInfo:telefone': ['^\\\\s?\\\\(?\\\\d{2,3}\\\\)?[\\\\s-]?\\\\d{4,5}-?\\\\d{4}\\\\s?$'],\n",
       " 'Datas:data': ['((0[1-9])|(1[0-9])|(2[0-9])|(3[0-1]))/((0[1-9])|(1[0-2]))/[0-9]{4}',\n",
       "  '((0[1-9])|(1[0-9])|(2[0-9])|(3[0-1]))-((0[1-9])|(1[0-2]))-[0-9]{4}',\n",
       "  '((0[1-9])|(1[0-2]))/[0-9]{4}',\n",
       "  '((0[1-9])|(1[0-2]))-[0-9]{4}',\n",
       "  '((0[1-9])|(1[0-2]))/[0-9]{2}'],\n",
       " 'CEP:cep': ['^\\\\s?\\\\d{5}-?\\\\d{3}\\\\s?$',\n",
       "  '([0-9]{5}-[0-9]{3}){1}',\n",
       "  '([0-9]{8}){1}']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.patterns_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patterns_matcher(patterns, sentence):\n",
    "    found_entities = []\n",
    "    for entity, values in patterns:\n",
    "        for value in values:\n",
    "            results = re.findall(value, sentence)\n",
    "            if results:\n",
    "                found_entities.append({entity: results})\n",
    "            else:\n",
    "                pass\n",
    "    return found_entities\n",
    "\n",
    "\"alterar gestor e tomar coffé clovesgtx@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ContactInfo:email': ['clovesgtx@gmail.com']},\n",
       " {'Datas:data': [('26', '', '', '26', '', '12', '', '12')]},\n",
       " {'Datas:data': [('12', '', '12')]},\n",
       " {'Datas:data': [('06', '06', '')]},\n",
       " {'Datas:data': [('12', '', '12')]},\n",
       " {'CEP:cep': ['98406-873']},\n",
       " {'CEP:cep': ['88054641']}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns = zip(index.patterns_entities.keys(), index.patterns_entities.values())\n",
    "text = \"e-mail: clovesgtx@gmail.com,  cep 88054641, meu telefone é 1198406-8732, aniversário 26/12/1989\"\n",
    "patterns_matcher(patterns=patterns, sentence=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN compound\n",
      "startup NOUN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Apple, is, looking, at, buying, U.K., startup, for, $, 1, billion]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "list(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('aipim', 'CulturaPlatacao:Mandioca'), ('macaxeira', 'CulturaPlatacao:Mandioca')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "# https://spacy.io/usage/processing-pipelines#custom-components\n",
    "class EntityMatcher(object):\n",
    "    name = \"entity_matcher\"\n",
    "\n",
    "    def __init__(self, nlp, terms, label):\n",
    "        patterns = [nlp.make_doc(text) for text in terms]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        self.matcher.add(label, None, *patterns)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        for match_id, start, end in matches:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]\n",
    "        return doc\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "terms = (\"macaxeira\", \"aipim\", \"castelinha\", \"uaipi\", \"mandioca-doce\", \n",
    "         \"mandioca-mansa\", \"maniva\", \"maniveira\", \"pão-de-pobre\", \n",
    "         \"mandioca-brava\", \"mandioca-amarga\")\n",
    "entity_matcher = EntityMatcher(nlp, terms, \"CulturaPlatacao:Mandioca\")\n",
    "\n",
    "nlp.add_pipe(entity_matcher, after=\"ner\")\n",
    "\n",
    "#print(nlp.pipe_names)  # The components in the pipeline\n",
    "\n",
    "doc = nlp(\"Essa raíz é chamada de diferente formas, como mandioca, aipim, macaxeira, etc\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "FUZZY_MATCH_THRESHOLD = 90\n",
    "TOKENIZER_QUERY = {\n",
    "  \"analyzer\": \"standard\",\n",
    "  \"text\" : \"\"\n",
    "}\n",
    " \n",
    "def SynonymMatcher(entity, value, sentence, sentence_tokens, found_entities):\n",
    "    for value in values:\n",
    "        words_of_entity  = value.split(\" \")\n",
    "        if len(words_of_entity) > 1:\n",
    "            score = fuzz.ratio(sentence, value)  \n",
    "            if score >= 90:\n",
    "                found_entities[entity] = sentence\n",
    "        else:\n",
    "            for token in sentence_tokens:\n",
    "                score = fuzz.ratio(token, value)  \n",
    "                if score >= 90:\n",
    "                    found_entities[entity] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elastic_db.elasticsearch import elastic_conection, NLPmodelIndex\n",
    "\n",
    "es = elastic_conection()\n",
    "\n",
    "index = NLPmodelIndex(es=es, workspace_id=\"dc1e7b3d-9137-4a20-a99c-d0d2029ef170\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(sentence):\n",
    "    TOKENIZER_QUERY[\"text\"] = sentence\n",
    "    sentence_tokens = es.indices.analyze(index=index.index_name, body=TOKENIZER_QUERY)\n",
    "    sentence_tokens = [item[\"token\"] for item in sentence_tokens[\"tokens\"]]\n",
    "    return sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Manager\n",
    "\n",
    "Pros = []\n",
    "manager = Manager()\n",
    "found_entities = manager.dict()\n",
    "sentence = \"madioca aipim macaxeira\"\n",
    "sentence_tokens = get_tokens(sentence)\n",
    "\n",
    "entities = zip(index.synonyms_entities.keys(), index.synonyms_entities.values())\n",
    "\n",
    "# Run this with a pool of 5 agents having a chunksize of 3 until finished\n",
    "# agents = len(dataset)\n",
    "# chunksize = 1\n",
    "for entity, values in entities:\n",
    "        p = Process(target=SynonymMatcher, args=(entity, values, sentence, sentence_tokens, found_entities))\n",
    "        Pros.append(p)\n",
    "        p.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntedidadesGerais:Agendamento de Férias ['Agendamento de Férias', 'Agendamento', 'Nova programação']\n",
      "EntedidadesGerais:Alteração da Data de Férias ['Alteração da Data de Férias', 'Alteração']\n",
      "EntedidadesGerais:Alteração de Cargo ['Alteração de Cargo']\n",
      "EntedidadesGerais:Alteração de Centro de Custo ['Alteração de Centro de Custo']\n",
      "EntedidadesGerais:Alteração de Gestor ['Alteração de Gestor']\n",
      "EntedidadesGerais:Alteração de Tipo de Contrato ['Alteração de Tipo de Contrato']\n",
      "EntedidadesGerais:Central de Serviço ['Central de Serviço']\n",
      "EntedidadesGerais:Enquadramento ['Enquadramento']\n",
      "EntedidadesGerais:Extensão do Contrato ['Extensão do Contrato']\n",
      "EntedidadesGerais:Mérito ['Mérito']\n",
      "EntedidadesGerais:Promoção ['Promoção']\n",
      "EntedidadesGerais:Transferência entre filiais ['Transferência entre filiais']\n",
      "DetalheDesconto:Cartão combustível ['Cartão combustível']\n",
      "DetalheDesconto:Cartão combustívelPensão Alimentícia ['Cartão combustívelPensão Alimentícia']\n",
      "DetalheDesconto:Empréstimos ['Empréstimos']\n",
      "DetalheDesconto:Faltas ['Faltas']\n",
      "DetalheDesconto:Farmácia ['Farmácia']\n",
      "DetalheDesconto:Gympass ['Gympass']\n",
      "DetalheDesconto:Multas ['Multas']\n",
      "DetalheDesconto:Outros itens relacionados a Folha de Pagamento ['Outros itens relacionados a Folha de Pagamento']\n",
      "DetalheDesconto:Pensão Alimentícia ['Pensão Alimentícia']\n",
      "DetalheDesconto:Plano de saúde/Odonto ['Plano de saúde/Odonto']\n",
      "DetalheDesconto:Previdência ['Previdência']\n",
      "DetalheDesconto:Refeição ['Refeição']\n",
      "DetalheDesconto:Seguros ['Seguros']\n",
      "DetalheDesconto:Transporte ['Transporte']\n",
      "DetalheDesconto:Vale Transporte ['Vale Transporte']\n",
      "DecimoTerceiro:décimo terceiro ['décimo terceiro', '13', '13º']\n",
      "CulturaPlatacao:Banana ['Banana']\n",
      "CulturaPlatacao:Café ['Café']\n",
      "CulturaPlatacao:Cana-de-Açúcar ['Cana-de-Açúcar']\n",
      "CulturaPlatacao:Milho ['Milho']\n",
      "CulturaPlatacao:Outras culturas ['Outras culturas']\n",
      "CulturaPlatacao:Soja ['Soja']\n",
      "DataProgramada:Data programada - hoje = 35 dias ou mais ['Data programada - hoje = 35 dias ou mais']\n",
      "DataProgramada:Data programada - hoje = menos de 35 dias ['Data programada - hoje = menos de 35 dias']\n",
      "EntidadeGeral:Verificar Solicitação ['Verificar Solicitação']\n",
      "ValidaWhats:Contato com a Central de Serviço do CoE ['Contato com a Central de Serviço do CoE']\n",
      "ValidaWhats:Preenchimento do Formulário no Portal do CoE ['Preenchimento do Formulário no Portal do CoE']\n",
      "Previdencia:Acompanhar Saldo ['Acompanhar Saldo']\n",
      "Previdencia:Aderir ao Benefício ['Aderir ao Benefício']\n",
      "Previdencia:Aderir o Benefício ['Aderir o Benefício']\n",
      "Previdencia:Cancelar Contribuições ['Cancelar Contribuições', 'Cancelamento de Contribuições']\n",
      "Previdencia:Colaboradores Inativos ['Colaboradores Inativos']\n",
      "Previdencia:Contatos da Operadora ['Contatos da Operadora']\n",
      "Previdencia:Contribuição Básica ['Contribuição Básica']\n",
      "Previdencia:Contribuição Complementar ['Contribuição Complementar']\n",
      "Previdencia:Contribuição Normal ['Contribuição Normal']\n",
      "Previdencia:Contribuições e Contrapartidas ['Contribuições e Contrapartidas']\n",
      "Previdencia:Descontos na Folha de Pagamento ['Descontos na Folha de Pagamento']\n",
      "Previdencia:Diferenças de PGBL e VGBL ['Diferenças de PGBL e VGBL']\n",
      "Previdencia:Diferenças PGBL e VGBL ['Diferenças PGBL e VGBL']\n",
      "Previdencia:Elegibilidades ao Plano ['Elegibilidades ao Plano']\n",
      "Previdencia:Formulários ['Formulários']\n",
      "Previdencia:Imposto de Renda ['Imposto de Renda']\n",
      "Previdencia:Plano de Colaboradores Inativos ['Plano de Colaboradores Inativos']\n",
      "Previdencia:Portabilidade ['Portabilidade']\n",
      "Previdencia:Recebimento do Benefício ['Recebimento do Benefício']\n",
      "Previdencia:Resgatar Valores ['Resgatar Valores']\n",
      "Previdencia:Tributação ['Tributação']\n",
      "Previdencia:Visão Geral ['Visão Geral']\n",
      "Previdencia:Visão Geral sobre Previdência Privada ['Visão Geral sobre Previdência Privada']\n",
      "Tema:Gestão de Frotas ['Gestão de Frotas', 'Frotas']\n",
      "Tema:Movimentações Funcionais ['Movimentações Funcionais']\n",
      "Tema:Plano de Saúde/Odontológico ['Plano de Saúde/Odontológico', 'Odontologico', 'Plano de Saúde', 'Plano Odontologico', 'Saude']\n",
      "Tema:PontoMais ['PontoMais', 'Ponto Mais']\n",
      "Tema:Previdência Privada ['Previdência Privada', 'Previdencia']\n",
      "Tema:Programação, Alteração e Cancelamento de Férias ['Programação, Alteração e Cancelamento de Férias', 'Alteração de Férias', 'Cancelamento de Férias', 'Programação de Férias']\n",
      "Tema:Reembolso, adiantamento e prestação de contas ['Reembolso, adiantamento e prestação de contas', 'adiantamento', 'prestação de contas', 'Reembolso']\n",
      "Tema:Salário, 13º e descontos ['Salário, 13º e descontos', '13º', 'decimo terceiro', 'descontos', 'Salário']\n",
      "Tema:Vale Refeição e Alimentação ['Vale Refeição e Alimentação', 'Vale Alimentação', 'Vale Refeição']\n",
      "Assistencias:Bradesco ['Bradesco']\n",
      "Assistencias:Central Nacional Unimed CNU ['Central Nacional Unimed CNU', 'CNU']\n",
      "Assistencias:Companheiro ['Companheiro']\n",
      "Assistencias:Cônjuge ['Cônjuge']\n",
      "Assistencias:Desejo obter o nº do cartão ou acessar o cartão digital ['Desejo obter o nº do cartão ou acessar o cartão digital']\n",
      "Assistencias:Desejo solicitar meu novo cartão físico ['Desejo solicitar meu novo cartão físico']\n",
      "Assistencias:Filhos ['Filhos']\n",
      "Assistencias:Filhos adotivos ['Filhos adotivos']\n",
      "Assistencias:Meu cartão vencerá em breve ['Meu cartão vencerá em breve']\n",
      "Assistencias:Meu cartão venceu e não recebi ['Meu cartão venceu e não recebi']\n",
      "Assistencias:Obter o nº do cartão ['Obter o nº do cartão']\n",
      "Assistencias:Outros ['Outros']\n",
      "Assistencias:Plano de Saude ['Plano de Saude']\n",
      "Assistencias:Plano Odontológico ['Plano Odontológico']\n",
      "Assistencias:solicitar a segunda via do cartão físico ['solicitar a segunda via do cartão físico']\n",
      "Assistencias:Unimed Campinas ['Unimed Campinas']\n",
      "Assistencias:Unimed POA ['Unimed POA', 'POA']\n"
     ]
    }
   ],
   "source": [
    "entities = zip(index.synonyms_entities.keys(), index.synonyms_entities.values())\n",
    "for entity, values in entities:\n",
    "    print(entity, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in Pros:\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_entities.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(es, index_name, value_type):\n",
    "    synonyms_result = es.search(index=index_name, body = {\n",
    "                                                          \"query\": {\n",
    "                                                           \"bool\": {\n",
    "                                                             \"must\": [\n",
    "                                                               {\"term\": {\"doc_type.keyword\": \"entity\"}},\n",
    "                                                               {\"term\": {\"values.type\": value_type}}\n",
    "                                                             ]\n",
    "                                                           }\n",
    "                                                          }\n",
    "                                                        })\n",
    "    \n",
    "    entities = [synonym[\"_source\"] for synonym in synonyms_result[\"hits\"][\"hits\"]]\n",
    "    dic = dict()\n",
    "    if value_type == \"synonyms\":\n",
    "        for entity in entities:\n",
    "            entity_name = entity[\"entity\"] + \":\"\n",
    "            for value in entity[\"values\"]:\n",
    "                key = entity_name+value[\"value\"]\n",
    "                dic[key] = [value[\"value\"]] + value[value_type]\n",
    "    if value_type == \"patterns\":\n",
    "        for entity in entities:\n",
    "            entity_name = entity[\"entity\"] + \":\"\n",
    "            for value in entity[\"values\"]:\n",
    "                key = entity_name+value[\"value\"]\n",
    "                dic[key] = value[value_type]\n",
    "\n",
    "    return dic\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CulturaPlatacao:Café': ['Café'],\n",
       " 'CulturaPlatacao:Cana-de-Açúcar': ['Cana-de-Açúcar', 'cana'],\n",
       " 'CulturaPlatacao:Mandioca': ['Mandioca',\n",
       "  'macaxeira',\n",
       "  'aipim',\n",
       "  'castelinha',\n",
       "  'uaipi',\n",
       "  'mandioca-doce',\n",
       "  'mandioca-mansa',\n",
       "  'maniva',\n",
       "  'maniveira',\n",
       "  'pão-de-pobre',\n",
       "  'mandioca-brava',\n",
       "  'mandioca-amarga']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_entities(es, index.index_name, \"synonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ContactInfo:email': ['\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{b,}\\\\b',\n",
       "  \"^[a-zA-Z0-9.!#$%&'*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\\\\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*$\"],\n",
       " 'ContactInfo:telefone': ['^\\\\s?\\\\(?\\\\d{2,3}\\\\)?[\\\\s-]?\\\\d{4,5}-?\\\\d{4}\\\\s?$']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_entities(es, index.index_name, \"patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x7f01137a5708>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(dic.keys(), dic.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['CulturaPlatacao:Café', 'CulturaPlatacao:Cana-de-Açúcar', 'CulturaPlatacao:Mandioca'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([['Café'], ['Cana-de-Açúcar', 'cana'], ['Mandioca', 'macaxeira', 'aipim', 'castelinha', 'uaipi', 'mandioca-doce', 'mandioca-mansa', 'maniva', 'maniveira', 'pão-de-pobre', 'mandioca-brava', 'mandioca-amarga']])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elastic_db.elasticsearch import NLPmodelIndex, elastic_conection\n",
    "from utils.entities import nlp, SynonymMatcher\n",
    "\n",
    "es = elastic_conection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = NLPmodelIndex(es=es, workspace_id=\"dc1e7b3d-9137-4a20-a99c-d0d2029ef170\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EntedidadesGerais:Agendamento de Férias': ['Agendamento de Férias',\n",
       "  'Agendamento',\n",
       "  'Nova programação'],\n",
       " 'EntedidadesGerais:Alteração da Data de Férias': ['Alteração da Data de Férias',\n",
       "  'Alteração'],\n",
       " 'EntedidadesGerais:Alteração de Cargo': ['Alteração de Cargo'],\n",
       " 'EntedidadesGerais:Alteração de Centro de Custo': ['Alteração de Centro de Custo'],\n",
       " 'EntedidadesGerais:Alteração de Gestor': ['Alteração de Gestor'],\n",
       " 'EntedidadesGerais:Alteração de Tipo de Contrato': ['Alteração de Tipo de Contrato'],\n",
       " 'EntedidadesGerais:Central de Serviço': ['Central de Serviço'],\n",
       " 'EntedidadesGerais:Enquadramento': ['Enquadramento'],\n",
       " 'EntedidadesGerais:Extensão do Contrato': ['Extensão do Contrato'],\n",
       " 'EntedidadesGerais:Mérito': ['Mérito'],\n",
       " 'EntedidadesGerais:Promoção': ['Promoção'],\n",
       " 'EntedidadesGerais:Transferência entre filiais': ['Transferência entre filiais'],\n",
       " 'DetalheDesconto:Cartão combustível': ['Cartão combustível'],\n",
       " 'DetalheDesconto:Cartão combustívelPensão Alimentícia': ['Cartão combustívelPensão Alimentícia'],\n",
       " 'DetalheDesconto:Empréstimos': ['Empréstimos'],\n",
       " 'DetalheDesconto:Faltas': ['Faltas'],\n",
       " 'DetalheDesconto:Farmácia': ['Farmácia'],\n",
       " 'DetalheDesconto:Gympass': ['Gympass'],\n",
       " 'DetalheDesconto:Multas': ['Multas'],\n",
       " 'DetalheDesconto:Outros itens relacionados a Folha de Pagamento': ['Outros itens relacionados a Folha de Pagamento'],\n",
       " 'DetalheDesconto:Pensão Alimentícia': ['Pensão Alimentícia'],\n",
       " 'DetalheDesconto:Plano de saúde/Odonto': ['Plano de saúde/Odonto'],\n",
       " 'DetalheDesconto:Previdência': ['Previdência'],\n",
       " 'DetalheDesconto:Refeição': ['Refeição'],\n",
       " 'DetalheDesconto:Seguros': ['Seguros'],\n",
       " 'DetalheDesconto:Transporte': ['Transporte'],\n",
       " 'DetalheDesconto:Vale Transporte': ['Vale Transporte'],\n",
       " 'DecimoTerceiro:décimo terceiro': ['décimo terceiro', '13', '13º'],\n",
       " 'CulturaPlatacao:Banana': ['Banana'],\n",
       " 'CulturaPlatacao:Café': ['Café'],\n",
       " 'CulturaPlatacao:Cana-de-Açúcar': ['Cana-de-Açúcar'],\n",
       " 'CulturaPlatacao:Milho': ['Milho'],\n",
       " 'CulturaPlatacao:Outras culturas': ['Outras culturas'],\n",
       " 'CulturaPlatacao:Soja': ['Soja'],\n",
       " 'DataProgramada:Data programada - hoje = 35 dias ou mais': ['Data programada - hoje = 35 dias ou mais'],\n",
       " 'DataProgramada:Data programada - hoje = menos de 35 dias': ['Data programada - hoje = menos de 35 dias'],\n",
       " 'EntidadeGeral:Verificar Solicitação': ['Verificar Solicitação'],\n",
       " 'ValidaWhats:Contato com a Central de Serviço do CoE': ['Contato com a Central de Serviço do CoE'],\n",
       " 'ValidaWhats:Preenchimento do Formulário no Portal do CoE': ['Preenchimento do Formulário no Portal do CoE'],\n",
       " 'Previdencia:Acompanhar Saldo': ['Acompanhar Saldo'],\n",
       " 'Previdencia:Aderir ao Benefício': ['Aderir ao Benefício'],\n",
       " 'Previdencia:Aderir o Benefício': ['Aderir o Benefício'],\n",
       " 'Previdencia:Cancelar Contribuições': ['Cancelar Contribuições',\n",
       "  'Cancelamento de Contribuições'],\n",
       " 'Previdencia:Colaboradores Inativos': ['Colaboradores Inativos'],\n",
       " 'Previdencia:Contatos da Operadora': ['Contatos da Operadora'],\n",
       " 'Previdencia:Contribuição Básica': ['Contribuição Básica'],\n",
       " 'Previdencia:Contribuição Complementar': ['Contribuição Complementar'],\n",
       " 'Previdencia:Contribuição Normal': ['Contribuição Normal'],\n",
       " 'Previdencia:Contribuições e Contrapartidas': ['Contribuições e Contrapartidas'],\n",
       " 'Previdencia:Descontos na Folha de Pagamento': ['Descontos na Folha de Pagamento'],\n",
       " 'Previdencia:Diferenças de PGBL e VGBL': ['Diferenças de PGBL e VGBL'],\n",
       " 'Previdencia:Diferenças PGBL e VGBL': ['Diferenças PGBL e VGBL'],\n",
       " 'Previdencia:Elegibilidades ao Plano': ['Elegibilidades ao Plano'],\n",
       " 'Previdencia:Formulários': ['Formulários'],\n",
       " 'Previdencia:Imposto de Renda': ['Imposto de Renda'],\n",
       " 'Previdencia:Plano de Colaboradores Inativos': ['Plano de Colaboradores Inativos'],\n",
       " 'Previdencia:Portabilidade': ['Portabilidade'],\n",
       " 'Previdencia:Recebimento do Benefício': ['Recebimento do Benefício'],\n",
       " 'Previdencia:Resgatar Valores': ['Resgatar Valores'],\n",
       " 'Previdencia:Tributação': ['Tributação'],\n",
       " 'Previdencia:Visão Geral': ['Visão Geral'],\n",
       " 'Previdencia:Visão Geral sobre Previdência Privada': ['Visão Geral sobre Previdência Privada'],\n",
       " 'Tema:Gestão de Frotas': ['Gestão de Frotas', 'Frotas'],\n",
       " 'Tema:Movimentações Funcionais': ['Movimentações Funcionais'],\n",
       " 'Tema:Plano de Saúde/Odontológico': ['Plano de Saúde/Odontológico',\n",
       "  'Odontologico',\n",
       "  'Plano de Saúde',\n",
       "  'Plano Odontologico',\n",
       "  'Saude'],\n",
       " 'Tema:PontoMais': ['PontoMais', 'Ponto Mais'],\n",
       " 'Tema:Previdência Privada': ['Previdência Privada', 'Previdencia'],\n",
       " 'Tema:Programação, Alteração e Cancelamento de Férias': ['Programação, Alteração e Cancelamento de Férias',\n",
       "  'Alteração de Férias',\n",
       "  'Cancelamento de Férias',\n",
       "  'Programação de Férias'],\n",
       " 'Tema:Reembolso, adiantamento e prestação de contas': ['Reembolso, adiantamento e prestação de contas',\n",
       "  'adiantamento',\n",
       "  'prestação de contas',\n",
       "  'Reembolso'],\n",
       " 'Tema:Salário, 13º e descontos': ['Salário, 13º e descontos',\n",
       "  '13º',\n",
       "  'decimo terceiro',\n",
       "  'descontos',\n",
       "  'Salário'],\n",
       " 'Tema:Vale Refeição e Alimentação': ['Vale Refeição e Alimentação',\n",
       "  'Vale Alimentação',\n",
       "  'Vale Refeição'],\n",
       " 'Assistencias:Bradesco': ['Bradesco'],\n",
       " 'Assistencias:Central Nacional Unimed CNU': ['Central Nacional Unimed CNU',\n",
       "  'CNU'],\n",
       " 'Assistencias:Companheiro': ['Companheiro'],\n",
       " 'Assistencias:Cônjuge': ['Cônjuge'],\n",
       " 'Assistencias:Desejo obter o nº do cartão ou acessar o cartão digital': ['Desejo obter o nº do cartão ou acessar o cartão digital'],\n",
       " 'Assistencias:Desejo solicitar meu novo cartão físico': ['Desejo solicitar meu novo cartão físico'],\n",
       " 'Assistencias:Filhos': ['Filhos'],\n",
       " 'Assistencias:Filhos adotivos': ['Filhos adotivos'],\n",
       " 'Assistencias:Meu cartão vencerá em breve': ['Meu cartão vencerá em breve'],\n",
       " 'Assistencias:Meu cartão venceu e não recebi': ['Meu cartão venceu e não recebi'],\n",
       " 'Assistencias:Obter o nº do cartão': ['Obter o nº do cartão'],\n",
       " 'Assistencias:Outros': ['Outros'],\n",
       " 'Assistencias:Plano de Saude': ['Plano de Saude'],\n",
       " 'Assistencias:Plano Odontológico': ['Plano Odontológico'],\n",
       " 'Assistencias:solicitar a segunda via do cartão físico': ['solicitar a segunda via do cartão físico'],\n",
       " 'Assistencias:Unimed Campinas': ['Unimed Campinas'],\n",
       " 'Assistencias:Unimed POA': ['Unimed POA', 'POA']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.synonyms_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ContactInfo:email': ['\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,}\\\\b',\n",
       "  \"^[a-zA-Z0-9.!#$%&'*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\\\\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*$\"],\n",
       " 'ContactInfo:telefone': ['^\\\\s?\\\\(?\\\\d{2,3}\\\\)?[\\\\s-]?\\\\d{4,5}-?\\\\d{4}\\\\s?$'],\n",
       " 'Datas:data': ['((0[1-9])|(1[0-9])|(2[0-9])|(3[0-1]))/((0[1-9])|(1[0-2]))/[0-9]{4}',\n",
       "  '((0[1-9])|(1[0-9])|(2[0-9])|(3[0-1]))-((0[1-9])|(1[0-2]))-[0-9]{4}',\n",
       "  '((0[1-9])|(1[0-2]))/[0-9]{4}',\n",
       "  '((0[1-9])|(1[0-2]))-[0-9]{4}',\n",
       "  '((0[1-9])|(1[0-2]))/[0-9]{2}'],\n",
       " 'CEP:cep': ['^\\\\s?\\\\d{5}-?\\\\d{3}\\\\s?$',\n",
       "  '([0-9]{5}-[0-9]{3}){1}',\n",
       "  '([0-9]{8}){1}']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.patterns_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_synonyms_matchers(index):\n",
    "    for synonym in index.synonyms_entities.keys():\n",
    "        entity_matcher = SynonymMatcher(nlp, index.synonyms_entities[synonym], synonym)\n",
    "        nlp.add_pipe(entity_matcher, after=\"ner\")\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E007] 'synonym_matcher' already exists in pipeline. Existing names: ['tagger', 'parser', 'ner', 'synonym_matcher']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-dc8180a8340c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_synonyms_matchers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-5d196ef6f158>\u001b[0m in \u001b[0;36mbuild_synonyms_matchers\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msynonym\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynonyms_entities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mentity_matcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSynonymMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynonyms_entities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msynonym\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynonym\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity_matcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, component, name, before, after, first, last)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_component_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE007\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mafter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE006\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E007] 'synonym_matcher' already exists in pipeline. Existing names: ['tagger', 'parser', 'ner', 'synonym_matcher']"
     ]
    }
   ],
   "source": [
    "nlp = build_synonyms_matchers(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
